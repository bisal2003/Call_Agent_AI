{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: groq in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (0.20.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from groq) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from groq) (2.10.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (2.27.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install groq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast language models are essential in today's technology landscape due to their ability to rapidly process and understand human language. The importance of fast language models can be understood from the following aspects:\n",
      "\n",
      "1. **Improved User Experience**: Fast language models enable applications to respond quickly to user queries, which leads to a better user experience. This is particularly important in applications such as virtual assistants, chatbots, and language translation software, where speed is crucial.\n",
      "2. **Real-Time Processing**: Fast language models can process and analyze large amounts of language data in real-time, enabling applications such as sentiment analysis, entity recognition, and topic modeling. This allows for immediate insights and decision-making.\n",
      "3. **Efficient Resource Utilization**: Fast language models can process language data more efficiently, reducing the computational resources required. This leads to cost savings and enables the deployment of language models on devices with limited resources, such as smartphones and edge devices.\n",
      "4. **Enhanced Accuracy**: Fast language models can process larger amounts of data, which can lead to improved accuracy in language understanding tasks. This is because fast models can consider more context and nuances of language, resulting in better performance.\n",
      "5. **Scalability**: Fast language models enable the development of large-scale language processing applications, such as language translation platforms, text summarization services, and content generation tools. This scalability is critical for applications that need to handle large volumes of language data.\n",
      "6. **Competitive Advantage**: In industries such as customer service, healthcare, and finance, fast language models can provide a competitive advantage by enabling organizations to respond quickly and effectively to customer inquiries, analyze large amounts of text data, and make informed decisions.\n",
      "7. **Research and Development**: Fast language models facilitate research and development in areas such as natural language processing (NLP), machine learning, and human-computer interaction. By enabling rapid prototyping and experimentation, fast language models accelerate innovation in these fields.\n",
      "8. **Accessibility**: Fast language models can be used to develop assistive technologies, such as language translation apps for people with disabilities, real-time captions for video conferencing, and speech recognition systems for individuals with speech disorders.\n",
      "\n",
      "To achieve fast language models, several techniques are employed, including:\n",
      "\n",
      "1. **Model pruning**: Removing unnecessary weights and connections in the model to reduce computational overhead.\n",
      "2. **Knowledge distillation**: Transferring knowledge from a large, pre-trained model to a smaller, faster model.\n",
      "3. **Quantization**: Representing model weights and activations using lower-precision data types to reduce memory usage and computational overhead.\n",
      "4. **Parallelization**: Distributing the computation across multiple processing units, such as GPUs or CPUs, to speed up processing.\n",
      "5. **Optimized algorithms**: Developing algorithms that are optimized for specific hardware architectures, such as GPUs or TPUs.\n",
      "\n",
      "In summary, fast language models are crucial for a wide range of applications, from improving user experience and enabling real-time processing to enhancing accuracy and scalability. By leveraging various techniques to optimize language model performance, developers can create fast, efficient, and accurate language models that drive innovation and growth in various industries.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001F3A6B33DD0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001F3A6B339E0>, model_name='llama-3.3-70b-versatile', temperature=0.9, model_kwargs={}, max_tokens=512)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../src\"))  # Add the src directory to sys.path\n",
    "\n",
    "from models import get_llm , gemini_llm # Now you can import the function\n",
    "\n",
    "get_llm()  # Call the function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGoogleGenerativeAI(model='models/gemini-1.5-flash', google_api_key=SecretStr('**********'), max_output_tokens=512, max_retries=2, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000001F3A6B30410>, default_metadata=())"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemini_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
