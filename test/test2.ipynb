{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: groq in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (0.13.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from groq) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from groq) (2.10.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (2.27.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install groq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "GroqError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the GROQ_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mGroqError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgroq\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Groq\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m client = \u001b[43mGroq\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43menviron\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGROQ_API_KEY\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m chat_completion = client.chat.completions.create(\n\u001b[32m     11\u001b[39m     messages=[\n\u001b[32m     12\u001b[39m         {\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mllama-3.3-70b-versatile\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     18\u001b[39m )\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(chat_completion.choices[\u001b[32m0\u001b[39m].message.content)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Asus\\Desktop\\Call_Agent_AI\\Call_Agent_AI\\.venv\\Lib\\site-packages\\groq\\_client.py:82\u001b[39m, in \u001b[36mGroq.__init__\u001b[39m\u001b[34m(self, api_key, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[39m\n\u001b[32m     80\u001b[39m     api_key = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mGROQ_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m GroqError(\n\u001b[32m     83\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe api_key client option must be set either by passing api_key to the client or by setting the GROQ_API_KEY environment variable\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     84\u001b[39m     )\n\u001b[32m     85\u001b[39m \u001b[38;5;28mself\u001b[39m.api_key = api_key\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m base_url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mGroqError\u001b[39m: The api_key client option must be set either by passing api_key to the client or by setting the GROQ_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from groq import Groq\n",
    "\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\Desktop\\Call_Agent_AI\\Call_Agent_AI\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000017DA936EC90>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000017DA940D6D0>, model_name='llama-3.3-70b-versatile', temperature=0.9, model_kwargs={}, groq_api_key=SecretStr('**********'), max_tokens=512)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../src\"))  # Add the src directory to sys.path\n",
    "\n",
    "from models import get_llm , gemini_llm # Now you can import the function\n",
    "\n",
    "get_llm()  # Call the function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Apple refers to a few things, most commonly:\\n\\n* **The fruit:**  A sweet, edible fruit produced by an apple tree (Malus domestica).  Many varieties exist, differing in size, color, flavor, and texture.\\n\\n* **Apple Inc.:** A multinational technology company headquartered in Cupertino, California.  They are known for their consumer electronics, software, and online services, including iPhones, iPads, Macs, and the App Store.\\n\\nThe context in which you heard or read the word \"apple\" will determine which meaning is intended.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-4b8f3d6b-e5f7-46e7-b2c5-85ca9448ca3a-0', usage_metadata={'input_tokens': 3, 'output_tokens': 115, 'total_tokens': 118, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemini_llm().invoke(\"what is apple\")  # Call the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sounddevice\n",
      "  Using cached sounddevice-0.5.1-py3-none-win_amd64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (1.15.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from sounddevice) (1.17.1)\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from scipy) (1.26.4)\n",
      "Requirement already satisfied: pycparser in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from CFFI>=1.0->sounddevice) (2.22)\n",
      "Using cached sounddevice-0.5.1-py3-none-win_amd64.whl (363 kB)\n",
      "Installing collected packages: sounddevice\n",
      "Successfully installed sounddevice-0.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sounddevice scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Recording complete\n",
      "Saved as recording.wav\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "# Configuration\n",
    "duration = 5  # seconds\n",
    "filename = \"recording.wav\"\n",
    "sample_rate = 44100  # standard audio sampling rate\n",
    "\n",
    "# Record audio\n",
    "print(\"Recording...\")\n",
    "recording = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1)\n",
    "sd.wait()  # Wait until recording is finished\n",
    "print(\"Recording complete\")\n",
    "\n",
    "# Save as WAV file\n",
    "write(filename, sample_rate, recording)\n",
    "print(f\"Saved as {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello, how are you? Hope you are fine\n",
      "2.043768882751465\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "import time\n",
    "# Initialize the Groq client\n",
    "\n",
    "start = time.time()\n",
    "client = Groq()\n",
    "\n",
    "# Specify the path to the audio file\n",
    "filename = \"recording.wav\" # Replace with your audio file!\n",
    "\n",
    "\n",
    "# Open the audio file\n",
    "with open(filename, \"rb\") as file:\n",
    "    # Create a transcription of the audio file\n",
    "    transcription = client.audio.transcriptions.create(\n",
    "      file=(filename, file.read()), # Required audio file\n",
    "      model=\"whisper-large-v3-turbo\", # Required model to use for transcription\n",
    "      prompt=\"Specify context or spelling\",  # Optional\n",
    "      response_format=\"json\",  # Optional\n",
    "      language=\"en\",  # Optional\n",
    "      temperature=0.0  # Optional\n",
    "    )\n",
    "    # Print the transcription text\n",
    "    print(transcription.text)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "os.environ[\"PATH\"] += os.pathsep + \"C:\\\\ffmpeg\\\\bin\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "';C:\\\\ffmpeg\\\\bin'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.pathsep + \"C:\\\\ffmpeg\\\\bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pydub import AudioSegment\n",
    "from pydub.utils import which\n",
    "\n",
    "os.environ[\"FFMPEG_BINARY\"] = \"C:\\\\Program Files\\\\ffmpeg\\\\bin\\\\ffmpeg.exe\"  # Update if needed\n",
    "AudioSegment.converter = which(\"ffmpeg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "AudioSegment.converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.utils import which\n",
    "\n",
    "AudioSegment.converter = which(\"ffmpeg\")\n",
    "AudioSegment.ffmpeg = which(\"ffmpeg\")\n",
    "AudioSegment.ffprobe = which(\"ffprobe\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "AudioSegment.converter = r\"C:\\ffmpeg.exe\"\n",
    "AudioSegment.ffprobe = r\"C:ffprobe.exe\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio successfully saved to c:\\Users\\Asus\\Desktop\\Call_Agent_AI\\Call_Agent_AI\\test\\frontend\\public\\assets\\female_8669b9b8-dc1a-4258-8a30-ae4f7322562b.mp3\n",
      "Error: [WinError 2] The system cannot find the file specified\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_23756\\3654170607.py\", line 43, in text_to_speech_female\n",
      "    audio = AudioSegment.from_mp3(output_file)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Asus\\Desktop\\Call_Agent_AI\\Call_Agent_AI\\.venv\\Lib\\site-packages\\pydub\\audio_segment.py\", line 796, in from_mp3\n",
      "    return cls.from_file(file, 'mp3', parameters=parameters)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Asus\\Desktop\\Call_Agent_AI\\Call_Agent_AI\\.venv\\Lib\\site-packages\\pydub\\audio_segment.py\", line 728, in from_file\n",
      "    info = mediainfo_json(orig_file, read_ahead_limit=read_ahead_limit)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Asus\\Desktop\\Call_Agent_AI\\Call_Agent_AI\\.venv\\Lib\\site-packages\\pydub\\utils.py\", line 274, in mediainfo_json\n",
      "    res = Popen(command, stdin=stdin_parameter, stdout=PIPE, stderr=PIPE)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [WinError 2] The system cannot find the file specified\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "from google.cloud import texttospeech\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "\n",
    "# Set Google Cloud credentials\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = r\"C:\\Users\\Asus\\Desktop\\Call_Agent_AI\\Call_Agent_AI\\neurosphere-453417-a13fa049f648.json\"\n",
    "\n",
    "def text_to_speech_female(text):\n",
    "    try:\n",
    "        # Initialize Google Cloud TTS Client\n",
    "        client = texttospeech.TextToSpeechClient()\n",
    "        synthesis_input = texttospeech.SynthesisInput(text=text)\n",
    "        uuid_ = uuid.uuid4()\n",
    "\n",
    "        # Path to save audio\n",
    "        output_dir = os.path.abspath(\"./frontend/public/assets/\")\n",
    "        if not os.path.exists(output_dir):\n",
    "            print(f\"Creating directory: {output_dir}\")\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        output_file = os.path.join(output_dir, f\"female_{uuid_}.mp3\")\n",
    "\n",
    "        # Voice configuration\n",
    "        voice = texttospeech.VoiceSelectionParams(\n",
    "            language_code=\"en-IN\",\n",
    "            name=\"en-IN-Chirp3-HD-Zephyr\",\n",
    "            ssml_gender=texttospeech.SsmlVoiceGender.FEMALE\n",
    "        )\n",
    "\n",
    "        audio_config = texttospeech.AudioConfig(audio_encoding=texttospeech.AudioEncoding.MP3)\n",
    "\n",
    "        # Generate speech\n",
    "        response = client.synthesize_speech(input=synthesis_input, voice=voice, audio_config=audio_config)\n",
    "\n",
    "        # Save the audio file\n",
    "        with open(output_file, \"wb\") as out:\n",
    "            out.write(response.audio_content)\n",
    "            print(f\"Audio successfully saved to {output_file}\")\n",
    "\n",
    "        # Play using pydub\n",
    "        audio = AudioSegment.from_mp3(output_file)\n",
    "        play(audio)\n",
    "\n",
    "        # Return the accessible path for React\n",
    "        return f\"/assets/female_{uuid_}.mp3\"\n",
    "\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(\"Error:\", e)\n",
    "        print(traceback.format_exc())\n",
    "        return None\n",
    "\n",
    "# Example Usage\n",
    "print(text_to_speech_female(\n",
    "    \"Hello Subrata, I hope you're doing well. It's always great to catch up. \"\n",
    "    \"Today, I wanted to share some exciting news about a new AI-powered tool that can generate realistic voice outputs. \"\n",
    "    \"It supports multiple languages and voice types, making it ideal for podcasts, audiobooks, or even interactive chat applications. \"\n",
    "    \"Let me know if you'd like to hear more about how it works!\"\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting soundfile\n",
      "  Using cached soundfile-0.13.1-py2.py3-none-win_amd64.whl.metadata (16 kB)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from soundfile) (1.17.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from soundfile) (1.26.4)\n",
      "Requirement already satisfied: pycparser in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from cffi>=1.0->soundfile) (2.22)\n",
      "Using cached soundfile-0.13.1-py2.py3-none-win_amd64.whl (1.0 MB)\n",
      "Installing collected packages: soundfile\n",
      "Successfully installed soundfile-0.13.1\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from datasets) (19.0.1)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.5.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from datasets) (0.27.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-3.4.1-py3-none-any.whl (487 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Using cached xxhash-3.5.0-cp312-cp312-win_amd64.whl (30 kB)\n",
      "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
      "Successfully installed datasets-3.4.1 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install soundfile\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "from groq import Groq\n",
    "from io import BytesIO\n",
    "import wave\n",
    "import threading\n",
    "import time\n",
    "import torch\n",
    "import soundfile as sf\n",
    "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\n",
    "from datasets import load_dataset\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_speech(text):\n",
    "        \"\"\"Convert text to speech and play it\"\"\"\n",
    "        inputs = tts_processor(text=text, return_tensors=\"pt\")\n",
    "        speech = tts_model.generate_speech(\n",
    "            inputs[\"input_ids\"], \n",
    "            speaker_embeddings, \n",
    "            vocoder=vocoder\n",
    "        )\n",
    "        \n",
    "        # Play audio directly\n",
    "        sd.play(speech.numpy(), samplerate=16000)\n",
    "        sd.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"Hello, how can I help you today?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tts_processor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtext_to_speech\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mtext_to_speech\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtext_to_speech\u001b[39m(text):\n\u001b[32m      2\u001b[39m \u001b[38;5;250m        \u001b[39m\u001b[33;03m\"\"\"Convert text to speech and play it\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m         inputs = \u001b[43mtts_processor\u001b[49m(text=text, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m         speech = tts_model.generate_speech(\n\u001b[32m      5\u001b[39m             inputs[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m], \n\u001b[32m      6\u001b[39m             speaker_embeddings, \n\u001b[32m      7\u001b[39m             vocoder=vocoder\n\u001b[32m      8\u001b[39m         )\n\u001b[32m     10\u001b[39m         \u001b[38;5;66;03m# Play audio directly\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'tts_processor' is not defined"
     ]
    }
   ],
   "source": [
    "text_to_speech(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nSpeechT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Load models\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m processor = \u001b[43mSpeechT5Processor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmicrosoft/speecht5_tts\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m model = SpeechT5ForTextToSpeech.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mmicrosoft/speecht5_tts\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m vocoder = SpeechT5HifiGan.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mmicrosoft/speecht5_hifigan\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Asus\\Desktop\\Call_Agent_AI\\Call_Agent_AI\\.venv\\Lib\\site-packages\\transformers\\processing_utils.py:974\u001b[39m, in \u001b[36mProcessorMixin.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[39m\n\u001b[32m    971\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    972\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtoken\u001b[39m\u001b[33m\"\u001b[39m] = token\n\u001b[32m--> \u001b[39m\u001b[32m974\u001b[39m args = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_arguments_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    975\u001b[39m processor_dict, kwargs = \u001b[38;5;28mcls\u001b[39m.get_processor_dict(pretrained_model_name_or_path, **kwargs)\n\u001b[32m    977\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.from_args_and_dict(args, processor_dict, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Asus\\Desktop\\Call_Agent_AI\\Call_Agent_AI\\.venv\\Lib\\site-packages\\transformers\\processing_utils.py:1020\u001b[39m, in \u001b[36mProcessorMixin._get_arguments_from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1017\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1018\u001b[39m         attribute_class = \u001b[38;5;28mgetattr\u001b[39m(transformers_module, class_name)\n\u001b[32m-> \u001b[39m\u001b[32m1020\u001b[39m     args.append(\u001b[43mattribute_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m(pretrained_model_name_or_path, **kwargs))\n\u001b[32m   1021\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m args\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Asus\\Desktop\\Call_Agent_AI\\Call_Agent_AI\\.venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1666\u001b[39m, in \u001b[36mDummyObject.__getattribute__\u001b[39m\u001b[34m(cls, key)\u001b[39m\n\u001b[32m   1664\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key.startswith(\u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key != \u001b[33m\"\u001b[39m\u001b[33m_from_config\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1665\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m1666\u001b[39m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Asus\\Desktop\\Call_Agent_AI\\Call_Agent_AI\\.venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1654\u001b[39m, in \u001b[36mrequires_backends\u001b[39m\u001b[34m(obj, backends)\u001b[39m\n\u001b[32m   1652\u001b[39m failed = [msg.format(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[32m   1653\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[32m-> \u001b[39m\u001b[32m1654\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(failed))\n",
      "\u001b[31mImportError\u001b[39m: \nSpeechT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sounddevice as sd\n",
    "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load models\n",
    "processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
    "\n",
    "# Load speaker embeddings\n",
    "embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
    "speaker_embeddings = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n",
    "\n",
    "def text_to_speech(text):\n",
    "    \"\"\"Convert text to speech and play audio.\"\"\"\n",
    "    inputs = processor(text=text, return_tensors=\"pt\")\n",
    "    speech = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings, vocoder=vocoder)\n",
    "\n",
    "    # Play audio\n",
    "    sd.play(speech.numpy(), samplerate=16000)\n",
    "    sd.wait()\n",
    "\n",
    "# Example usage\n",
    "text = \"Hello, this is a text-to-speech test.\"\n",
    "text_to_speech(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"this is an apple\"\n",
    "\n",
    "def text_to_speech(text):\n",
    "    try:\n",
    "        inputs = tts_processor(text=text, return_tensors=\"pt\")\n",
    "        speech = tts_model.generate_speech(\n",
    "            inputs[\"input_ids\"], \n",
    "            speaker_embeddings, \n",
    "            vocoder=vocoder\n",
    "        )\n",
    "        \n",
    "        # Save to file with timestamp\n",
    "        filename = f\"response.wav\"\n",
    "        sf.write(filename, speech.numpy(), SAMPLE_RATE)\n",
    "        print(f\"Saved response as {filename}\")\n",
    "        \n",
    "        data, samplerate = sf.read('response.wav')\n",
    "\n",
    "        # Play the audio\n",
    "        sd.play(data, samplerate)\n",
    "        sd.wait()\n",
    "        return Audio(filename)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"TTS Error: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTS Error: name 'tts_processor' is not defined\n"
     ]
    }
   ],
   "source": [
    "text_to_speech(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
