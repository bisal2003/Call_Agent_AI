{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: groq in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (0.20.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from groq) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from groq) (2.10.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (2.27.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install groq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast language models are essential in today's technology landscape due to their ability to rapidly process and understand human language. The importance of fast language models can be understood from the following aspects:\n",
      "\n",
      "1. **Improved User Experience**: Fast language models enable applications to respond quickly to user queries, which leads to a better user experience. This is particularly important in applications such as virtual assistants, chatbots, and language translation software, where speed is crucial.\n",
      "2. **Real-Time Processing**: Fast language models can process and analyze large amounts of language data in real-time, enabling applications such as sentiment analysis, entity recognition, and topic modeling. This allows for immediate insights and decision-making.\n",
      "3. **Efficient Resource Utilization**: Fast language models can process language data more efficiently, reducing the computational resources required. This leads to cost savings and enables the deployment of language models on devices with limited resources, such as smartphones and edge devices.\n",
      "4. **Enhanced Accuracy**: Fast language models can process larger amounts of data, which can lead to improved accuracy in language understanding tasks. This is because fast models can consider more context and nuances of language, resulting in better performance.\n",
      "5. **Scalability**: Fast language models enable the development of large-scale language processing applications, such as language translation platforms, text summarization services, and content generation tools. This scalability is critical for applications that need to handle large volumes of language data.\n",
      "6. **Competitive Advantage**: In industries such as customer service, healthcare, and finance, fast language models can provide a competitive advantage by enabling organizations to respond quickly and effectively to customer inquiries, analyze large amounts of text data, and make informed decisions.\n",
      "7. **Research and Development**: Fast language models facilitate research and development in areas such as natural language processing (NLP), machine learning, and human-computer interaction. By enabling rapid prototyping and experimentation, fast language models accelerate innovation in these fields.\n",
      "8. **Accessibility**: Fast language models can be used to develop assistive technologies, such as language translation apps for people with disabilities, real-time captions for video conferencing, and speech recognition systems for individuals with speech disorders.\n",
      "\n",
      "To achieve fast language models, several techniques are employed, including:\n",
      "\n",
      "1. **Model pruning**: Removing unnecessary weights and connections in the model to reduce computational overhead.\n",
      "2. **Knowledge distillation**: Transferring knowledge from a large, pre-trained model to a smaller, faster model.\n",
      "3. **Quantization**: Representing model weights and activations using lower-precision data types to reduce memory usage and computational overhead.\n",
      "4. **Parallelization**: Distributing the computation across multiple processing units, such as GPUs or CPUs, to speed up processing.\n",
      "5. **Optimized algorithms**: Developing algorithms that are optimized for specific hardware architectures, such as GPUs or TPUs.\n",
      "\n",
      "In summary, fast language models are crucial for a wide range of applications, from improving user experience and enabling real-time processing to enhancing accuracy and scalability. By leveraging various techniques to optimize language model performance, developers can create fast, efficient, and accurate language models that drive innovation and growth in various industries.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001F3A6B33DD0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001F3A6B339E0>, model_name='llama-3.3-70b-versatile', temperature=0.9, model_kwargs={}, max_tokens=512)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../src\"))  # Add the src directory to sys.path\n",
    "\n",
    "from models import get_llm , gemini_llm # Now you can import the function\n",
    "\n",
    "get_llm()  # Call the function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGoogleGenerativeAI(model='models/gemini-1.5-flash', google_api_key=SecretStr('**********'), max_output_tokens=512, max_retries=2, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000001F3A6B30410>, default_metadata=())"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemini_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sounddevice\n",
      "  Using cached sounddevice-0.5.1-py3-none-win_amd64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (1.15.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from sounddevice) (1.17.1)\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from scipy) (1.26.4)\n",
      "Requirement already satisfied: pycparser in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from CFFI>=1.0->sounddevice) (2.22)\n",
      "Using cached sounddevice-0.5.1-py3-none-win_amd64.whl (363 kB)\n",
      "Installing collected packages: sounddevice\n",
      "Successfully installed sounddevice-0.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sounddevice scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Recording complete\n",
      "Saved as recording.wav\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "# Configuration\n",
    "duration = 5  # seconds\n",
    "filename = \"recording.wav\"\n",
    "sample_rate = 44100  # standard audio sampling rate\n",
    "\n",
    "# Record audio\n",
    "print(\"Recording...\")\n",
    "recording = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1)\n",
    "sd.wait()  # Wait until recording is finished\n",
    "print(\"Recording complete\")\n",
    "\n",
    "# Save as WAV file\n",
    "write(filename, sample_rate, recording)\n",
    "print(f\"Saved as {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello, how are you? Hope you are fine\n",
      "2.043768882751465\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "import time\n",
    "# Initialize the Groq client\n",
    "\n",
    "start = time.time()\n",
    "client = Groq()\n",
    "\n",
    "# Specify the path to the audio file\n",
    "filename = \"recording.wav\" # Replace with your audio file!\n",
    "\n",
    "\n",
    "# Open the audio file\n",
    "with open(filename, \"rb\") as file:\n",
    "    # Create a transcription of the audio file\n",
    "    transcription = client.audio.transcriptions.create(\n",
    "      file=(filename, file.read()), # Required audio file\n",
    "      model=\"whisper-large-v3-turbo\", # Required model to use for transcription\n",
    "      prompt=\"Specify context or spelling\",  # Optional\n",
    "      response_format=\"json\",  # Optional\n",
    "      language=\"en\",  # Optional\n",
    "      temperature=0.0  # Optional\n",
    "    )\n",
    "    # Print the transcription text\n",
    "    print(transcription.text)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "os.environ[\"PATH\"] += os.pathsep + \"C:\\\\ffmpeg\\\\bin\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "';C:\\\\ffmpeg\\\\bin'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.pathsep + \"C:\\\\ffmpeg\\\\bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pydub import AudioSegment\n",
    "from pydub.utils import which\n",
    "\n",
    "os.environ[\"FFMPEG_BINARY\"] = \"C:\\\\Program Files\\\\ffmpeg\\\\bin\\\\ffmpeg.exe\"  # Update if needed\n",
    "AudioSegment.converter = which(\"ffmpeg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "AudioSegment.converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.utils import which\n",
    "\n",
    "AudioSegment.converter = which(\"ffmpeg\")\n",
    "AudioSegment.ffmpeg = which(\"ffmpeg\")\n",
    "AudioSegment.ffprobe = which(\"ffprobe\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "AudioSegment.converter = r\"C:\\ffmpeg.exe\"\n",
    "AudioSegment.ffprobe = r\"C:ffprobe.exe\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio successfully saved to c:\\Users\\Asus\\Desktop\\Call_Agent_AI\\Call_Agent_AI\\test\\frontend\\public\\assets\\female_8669b9b8-dc1a-4258-8a30-ae4f7322562b.mp3\n",
      "Error: [WinError 2] The system cannot find the file specified\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_23756\\3654170607.py\", line 43, in text_to_speech_female\n",
      "    audio = AudioSegment.from_mp3(output_file)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Asus\\Desktop\\Call_Agent_AI\\Call_Agent_AI\\.venv\\Lib\\site-packages\\pydub\\audio_segment.py\", line 796, in from_mp3\n",
      "    return cls.from_file(file, 'mp3', parameters=parameters)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Asus\\Desktop\\Call_Agent_AI\\Call_Agent_AI\\.venv\\Lib\\site-packages\\pydub\\audio_segment.py\", line 728, in from_file\n",
      "    info = mediainfo_json(orig_file, read_ahead_limit=read_ahead_limit)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Asus\\Desktop\\Call_Agent_AI\\Call_Agent_AI\\.venv\\Lib\\site-packages\\pydub\\utils.py\", line 274, in mediainfo_json\n",
      "    res = Popen(command, stdin=stdin_parameter, stdout=PIPE, stderr=PIPE)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [WinError 2] The system cannot find the file specified\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "from google.cloud import texttospeech\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "\n",
    "# Set Google Cloud credentials\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = r\"C:\\Users\\Asus\\Desktop\\Call_Agent_AI\\Call_Agent_AI\\neurosphere-453417-a13fa049f648.json\"\n",
    "\n",
    "def text_to_speech_female(text):\n",
    "    try:\n",
    "        # Initialize Google Cloud TTS Client\n",
    "        client = texttospeech.TextToSpeechClient()\n",
    "        synthesis_input = texttospeech.SynthesisInput(text=text)\n",
    "        uuid_ = uuid.uuid4()\n",
    "\n",
    "        # Path to save audio\n",
    "        output_dir = os.path.abspath(\"./frontend/public/assets/\")\n",
    "        if not os.path.exists(output_dir):\n",
    "            print(f\"Creating directory: {output_dir}\")\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        output_file = os.path.join(output_dir, f\"female_{uuid_}.mp3\")\n",
    "\n",
    "        # Voice configuration\n",
    "        voice = texttospeech.VoiceSelectionParams(\n",
    "            language_code=\"en-IN\",\n",
    "            name=\"en-IN-Chirp3-HD-Zephyr\",\n",
    "            ssml_gender=texttospeech.SsmlVoiceGender.FEMALE\n",
    "        )\n",
    "\n",
    "        audio_config = texttospeech.AudioConfig(audio_encoding=texttospeech.AudioEncoding.MP3)\n",
    "\n",
    "        # Generate speech\n",
    "        response = client.synthesize_speech(input=synthesis_input, voice=voice, audio_config=audio_config)\n",
    "\n",
    "        # Save the audio file\n",
    "        with open(output_file, \"wb\") as out:\n",
    "            out.write(response.audio_content)\n",
    "            print(f\"Audio successfully saved to {output_file}\")\n",
    "\n",
    "        # Play using pydub\n",
    "        audio = AudioSegment.from_mp3(output_file)\n",
    "        play(audio)\n",
    "\n",
    "        # Return the accessible path for React\n",
    "        return f\"/assets/female_{uuid_}.mp3\"\n",
    "\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(\"Error:\", e)\n",
    "        print(traceback.format_exc())\n",
    "        return None\n",
    "\n",
    "# Example Usage\n",
    "print(text_to_speech_female(\n",
    "    \"Hello Subrata, I hope you're doing well. It's always great to catch up. \"\n",
    "    \"Today, I wanted to share some exciting news about a new AI-powered tool that can generate realistic voice outputs. \"\n",
    "    \"It supports multiple languages and voice types, making it ideal for podcasts, audiobooks, or even interactive chat applications. \"\n",
    "    \"Let me know if you'd like to hear more about how it works!\"\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting soundfile\n",
      "  Using cached soundfile-0.13.1-py2.py3-none-win_amd64.whl.metadata (16 kB)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from soundfile) (1.17.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from soundfile) (1.26.4)\n",
      "Requirement already satisfied: pycparser in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from cffi>=1.0->soundfile) (2.22)\n",
      "Using cached soundfile-0.13.1-py2.py3-none-win_amd64.whl (1.0 MB)\n",
      "Installing collected packages: soundfile\n",
      "Successfully installed soundfile-0.13.1\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from datasets) (19.0.1)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.5.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from datasets) (0.27.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-3.4.1-py3-none-any.whl (487 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Using cached xxhash-3.5.0-cp312-cp312-win_amd64.whl (30 kB)\n",
      "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
      "Successfully installed datasets-3.4.1 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install soundfile\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "from groq import Groq\n",
    "from io import BytesIO\n",
    "import wave\n",
    "import threading\n",
    "import time\n",
    "import torch\n",
    "import soundfile as sf\n",
    "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\n",
    "from datasets import load_dataset\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_speech(text):\n",
    "        \"\"\"Convert text to speech and play it\"\"\"\n",
    "        inputs = tts_processor(text=text, return_tensors=\"pt\")\n",
    "        speech = tts_model.generate_speech(\n",
    "            inputs[\"input_ids\"], \n",
    "            speaker_embeddings, \n",
    "            vocoder=vocoder\n",
    "        )\n",
    "        \n",
    "        # Play audio directly\n",
    "        sd.play(speech.numpy(), samplerate=16000)\n",
    "        sd.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"Hello, how can I help you today?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tts_processor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtext_to_speech\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mtext_to_speech\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtext_to_speech\u001b[39m(text):\n\u001b[32m      2\u001b[39m \u001b[38;5;250m        \u001b[39m\u001b[33;03m\"\"\"Convert text to speech and play it\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m         inputs = \u001b[43mtts_processor\u001b[49m(text=text, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m         speech = tts_model.generate_speech(\n\u001b[32m      5\u001b[39m             inputs[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m], \n\u001b[32m      6\u001b[39m             speaker_embeddings, \n\u001b[32m      7\u001b[39m             vocoder=vocoder\n\u001b[32m      8\u001b[39m         )\n\u001b[32m     10\u001b[39m         \u001b[38;5;66;03m# Play audio directly\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'tts_processor' is not defined"
     ]
    }
   ],
   "source": [
    "text_to_speech(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\asus\\desktop\\call_agent_ai\\call_agent_ai\\.venv\\lib\\site-packages (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nSpeechT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Load models\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m processor = \u001b[43mSpeechT5Processor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmicrosoft/speecht5_tts\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m model = SpeechT5ForTextToSpeech.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mmicrosoft/speecht5_tts\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m vocoder = SpeechT5HifiGan.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mmicrosoft/speecht5_hifigan\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Asus\\Desktop\\Call_Agent_AI\\Call_Agent_AI\\.venv\\Lib\\site-packages\\transformers\\processing_utils.py:974\u001b[39m, in \u001b[36mProcessorMixin.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[39m\n\u001b[32m    971\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    972\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtoken\u001b[39m\u001b[33m\"\u001b[39m] = token\n\u001b[32m--> \u001b[39m\u001b[32m974\u001b[39m args = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_arguments_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    975\u001b[39m processor_dict, kwargs = \u001b[38;5;28mcls\u001b[39m.get_processor_dict(pretrained_model_name_or_path, **kwargs)\n\u001b[32m    977\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.from_args_and_dict(args, processor_dict, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Asus\\Desktop\\Call_Agent_AI\\Call_Agent_AI\\.venv\\Lib\\site-packages\\transformers\\processing_utils.py:1020\u001b[39m, in \u001b[36mProcessorMixin._get_arguments_from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1017\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1018\u001b[39m         attribute_class = \u001b[38;5;28mgetattr\u001b[39m(transformers_module, class_name)\n\u001b[32m-> \u001b[39m\u001b[32m1020\u001b[39m     args.append(\u001b[43mattribute_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m(pretrained_model_name_or_path, **kwargs))\n\u001b[32m   1021\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m args\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Asus\\Desktop\\Call_Agent_AI\\Call_Agent_AI\\.venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1666\u001b[39m, in \u001b[36mDummyObject.__getattribute__\u001b[39m\u001b[34m(cls, key)\u001b[39m\n\u001b[32m   1664\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key.startswith(\u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key != \u001b[33m\"\u001b[39m\u001b[33m_from_config\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1665\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m1666\u001b[39m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Asus\\Desktop\\Call_Agent_AI\\Call_Agent_AI\\.venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1654\u001b[39m, in \u001b[36mrequires_backends\u001b[39m\u001b[34m(obj, backends)\u001b[39m\n\u001b[32m   1652\u001b[39m failed = [msg.format(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[32m   1653\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[32m-> \u001b[39m\u001b[32m1654\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(failed))\n",
      "\u001b[31mImportError\u001b[39m: \nSpeechT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sounddevice as sd\n",
    "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load models\n",
    "processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
    "\n",
    "# Load speaker embeddings\n",
    "embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
    "speaker_embeddings = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n",
    "\n",
    "def text_to_speech(text):\n",
    "    \"\"\"Convert text to speech and play audio.\"\"\"\n",
    "    inputs = processor(text=text, return_tensors=\"pt\")\n",
    "    speech = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings, vocoder=vocoder)\n",
    "\n",
    "    # Play audio\n",
    "    sd.play(speech.numpy(), samplerate=16000)\n",
    "    sd.wait()\n",
    "\n",
    "# Example usage\n",
    "text = \"Hello, this is a text-to-speech test.\"\n",
    "text_to_speech(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"this is an apple\"\n",
    "\n",
    "def text_to_speech(text):\n",
    "    try:\n",
    "        inputs = tts_processor(text=text, return_tensors=\"pt\")\n",
    "        speech = tts_model.generate_speech(\n",
    "            inputs[\"input_ids\"], \n",
    "            speaker_embeddings, \n",
    "            vocoder=vocoder\n",
    "        )\n",
    "        \n",
    "        # Save to file with timestamp\n",
    "        filename = f\"response.wav\"\n",
    "        sf.write(filename, speech.numpy(), SAMPLE_RATE)\n",
    "        print(f\"Saved response as {filename}\")\n",
    "        \n",
    "        data, samplerate = sf.read('response.wav')\n",
    "\n",
    "        # Play the audio\n",
    "        sd.play(data, samplerate)\n",
    "        sd.wait()\n",
    "        return Audio(filename)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"TTS Error: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTS Error: name 'tts_processor' is not defined\n"
     ]
    }
   ],
   "source": [
    "text_to_speech(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
